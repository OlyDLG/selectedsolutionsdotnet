<style>
body {font-family: Palatino}
</style>
#### <center>Ross, S., 1992. <i>Applied Probability Models with Optimization Applications.</i>  Dover, New York
#### <center>Chapter 2 Selected Problem Solutions
#### <center>&copy; 2016 by 
#### <center>David Lawrence Goldsmith
#### <center>for
#### <center>www.selectedsolutions.net

1) Show that Def. 2.1 implies Def. 2.2.

Soln.: We must show that, given a counting process $\{N(t), t \ge 0\}$ (henceforth $t \ge 0$ is understood unless specified otherwise) such that: i) $N(0) = 0$; ii) $\{N(t)\}$ has ind. incr.; and iii) $P\{N(t+s) - N(s) = n\} = e^{-\lambda t}(\lambda t)^n/n!$ $\forall n \in \mathbb{W}, s,t \in \mathbb{R} \ge 0$, it follows that: i) $\{N(t)\}$ has stat. incr.; ii) $P\{N(t) \ge 2\} = o(t)$; and iii) $P\{N(t) = 1\} = \lambda t + o(t).~$ (We have omitted Def. 2.2, Assumption (i) and ind. incr. since these are true by assumption given Def. 2.1.)

To show stat. incr., choose, w.l.o.g., $0 \le t_1 \le t_2$ and observe that  $P\{N(t_2) - N(t_1) \le n\} = P\{N(t_1 + t_2 - t_1) - N(t_1) \le n\} = P\{N(t_1 + s) - N(t_1) \le n\}$ (where $s =t_2 - t_1 \ge 0$) $= \sum_{i=0}^n P\{N(t_1 + s) - N(t_1) = i\} = \sum_{i=0}^n e^{-\lambda s} (\lambda s)^i/i!.$&nbsp; Thus, as far as $t$ is concerned, $P$ depends only on $s = t_2 - t_1$, i.e., the distance between two times, not their specific values, which is the verbal description of stat. incr.

To show $P\{N(t) \ge 2\} = o(t)$, we have: $P\{N(t) \ge 2\} = 1 - P\{N(t) = 0\} - P\{N(t) = 1\} = 1 - e^{-\lambda t} - \lambda t e^{-\lambda t}$ (these follow from Def. 2.1, Assumption (iii) by setting $s=0$ and then using Assumption (i)) $= 1 - e^{-\lambda t}(1+\lambda t)$,

$\therefore \lim_{t \rightarrow 0} \frac{P\{N(t) \ge 2\}}{t} = \lim_{t \rightarrow 0} \frac{1 - e^{-\lambda t}(1+\lambda t)}{t} "=" \frac00$, so, using L'Hopital's Rule, we find

$\lim_{t \rightarrow 0} \frac{1 - e^{-\lambda t}(1+\lambda t)}{t} = \lim_{t \rightarrow 0} \frac{-\lambda e^{-\lambda t} + \lambda(1+\lambda t)e^{-\lambda t}}{1} = \lim_{t \rightarrow 0} \lambda^2te^{-\lambda t} = 0$, as required.

Finally, to show $P\{N(t) = 1\} = \lambda t + o(t)$, we have: 

$\lim_{t \rightarrow 0} \frac{P\{N(t) = 1\} - \lambda t}{t} = \lim_{t \rightarrow 0} \frac{\lambda t e^{-\lambda t}- \lambda t}{t} = \lim_{t \rightarrow 0} \lambda (e^{-\lambda t} - 1) = 0$.  

QED

<br>
2.2) Show that Assumption (iv) of Def. 2.2 follows from Assumptions (ii) & (iii).

Soln.: We need to show that $\{N(t)\}$ having stat., ind. incr. and $P\{N(t) \ge 2\} = o(t)$ together imply $P\{N(t) = 1\} = \lambda t + o(t).~$ We start by writing what we do know about $P\{N(t) = 1\}$, namely:
$P\{N(t) = 1\} = 1 - P\{N(t) = 0\} - P\{N(t) \ge 2\} =$ (by Assum. (iv)) $1 - P\{N(t) = 0\} + o(t).~$ 
Let $f(t) = P\{N(t) = 0\} = P\{(\text{count up to time }s \lt t)=0~\&~(\text{count from time }s\text{ to }t) = 0\}$

$= P\{N(s) = 0, N(t-s) = 0\}$ ((count from time $s$ to $t$) $= N(t-s)$ by stat. incr. assumption)

$=\text{(by ind. incr.) }P\{N(s) = 0\}P\{N(t-s) = 0\}$, or, by making the variable change $u = t-s,~P\{N(s+u) = 0\} = P\{N(s) = 0\}P\{N(u) = 0\}$, i.e., $f(s+u) = f(s)f(u)~\forall s,u \in \mathbb{R} \gt 0.~$ But by a result from Chapter 1, this last implies that $f(t) = e^{-\lambda t}$ for some real constant $\lambda.~$ Thus we have that:

$P\{N(t) = 1\} = 1 - e^{-\lambda t} + o(t) = 1 - [1 - \lambda t + o(t^2)] + o(t) = \lambda t + o(t)$, as was to be shown.

QED

<br>
2.3) Suppose that each event has probability $p$ of being registered, independently of other events.&nbsp; If the event generating process is Poisson with rate $\lambda$, show that the counting process of registered events is Poisson with rate $\lambda p$.

Soln.: Let $\{G(t)\}$ represent the generating counting process and $\{R(t)\}$ the registering counting process; we must show that: $R(0) = 0$; $\{R(t)\}$ has ind. incr.; and either: A) Assumption (iii) of Def. 2.1 holds, or B) $\{R(t)\}$ also has stat. incr., and Assumption (iii) of Def. 2.2 holds (by the result of Problem 2.2, Assumption (iv) of Def. 2.2 holds if Assumptions (ii) and (iii) do).  We shall follow approach A), but first: note that $G(t) \ge R(t)$ $\forall t$ (why?) and thus since $G(0) = 0$ and $R(t) \ge 0$ $\forall t$ (why?), $R(0) = 0$.

To show ind. incr., w.l.o.g., choose $t_i, i=0,1,...,n : t_{i-1} \lt t_i \forall i=1,...,n$; we must show that $P\{R(t_i) - R(t_{i-1}) \le N, R(t_j) - R(t_{j-1}) \le M\} = P\{R(t_i) - R(t_{i-1}) \le N\}P\{R(t_j) - R(t_{j-1}) \le M\} \forall i, j = 1,...,n, i \ne j, M, N \in \mathbb{W}.~$ Now $P\{R(t_i) - R(t_{i-1}) \le N, R(t_j) - R(t_{j-1}) \le M\} = P\{G(t_i) - n_i - G(t_{i-1}) + n_{i-1} \le N, G(t_j) - n_j - G(t_{j-1}) + n_{j-1} \le M\}$ (where each of the $n_{k}$ are $\in \mathbb{W}$; why?) $\therefore$ $= P\{G(t_i) - G(t_{i-1}) \le N + n_i - n_{i-1}, G(t_j) - G(t_{j-1}) \le M + n_j - n_{j-1}\}.~$ Now, since $G$ is Poisson by assumption, this last does $= P\{G(t_i) - G(t_{i-1}) \le N + n_i - n_{i-1}\} P\{G(t_j) - G(t_{j-1}) \le M + n_j - n_{j-1}\}$
$= P\{G(t_i) - n_i - G(t_{i-1}) + n_{i-1} \le N\} P\{G(t_j) - n_j - G(t_{j-1}) + n_{j-1} \le M\}$
$= P\{R(t_i) - R(t_{i-1}) \le N\}P\{R(t_j) - R(t_{j-1}) \le M\}$, as required.

Finally, to show that Assumption (iii) of Def. 2.1 holds, we argue as follows: $R(t) = n \Rightarrow G(t) \ge n$ (why?), so

$P\{R(t) = n\} = P\{R(t) = n, G(t) = n\} + P\{R(t) = n, G(t) = n+1\}+~...= \sum_{i=n}^{\infty} P\{R(t) = n, G(t) = i\}$
$= \sum_{i=n}^{\infty} P\{R(t) = n \mid G(t) = i\}P\{G(t) = i\} = \sum_{i=n}^{\infty} (e^{-\lambda t}(\lambda t)^i/i!) P\{R(t) = n \mid G(t) = i\}$...but what is $P\{R(t) = n \mid G(t) = i\}$?&nbsp; This is the probability that, out of $i$ events generated, $n$ are registered, where each event has a probability $p$ of being registered (and, presumably, is either registered or not, i.e., we assume no "partial" registrations).&nbsp; But this describes a binomial experiment, where we're asking for the probability of $n$ "successes" out of $i$ "trials" where the probability of "success" is $p.~$ Thus $P\{R(t) = n \mid G(t) = i\} = \left(\!
  \begin{array}{c}
    i \\
    n
  \end{array}
  \!\right) p^n (1-p)^{i-n}.~$ Setting $q=1-p$, we have:

$P\{R(t) = n\} = \sum_{i=n}^{\infty} e^{-\lambda t}(\lambda t)^i/i! \left(\!
  \begin{array}{c}
    i \\
    n
  \end{array}
  \!\right) p^n q^{i-n} = e^{-\lambda t}\sum_{i=0}^{\infty} (\lambda t)^{i+n}/(i+n)! \left(\!
  \begin{array}{c}
    i + n \\
    n
  \end{array}
  \!\right) p^n q^{i+n-n}$
$= e^{-\lambda t}(p\lambda t)^n /n!\sum_{i=0}^{\infty} (q\lambda t)^i/i! = e^{\lambda t - p\lambda t}e^{-\lambda t}(p\lambda t)^n /n! = e^{-p\lambda t}(p\lambda t)^n/n!$

QED

<br>
Bonus Problem: The result of Problem 2.3 is "intuitive": it basically says that if the register has only a probability $p$ of registering an event, that's the same as if it had a probability 1 of registering, but only operates $pt$ of the time, thus proportionately reducing the count of events, "on average," but without changing the underlying, i.e., Poisson, nature of the process.

This "begs the question": does this same heuristic analysis of such a register apply to other underlying distributions, i.e., does it similarly simply reduce the "mean registration incidence" proportionately, without changing the distributional nature of the counting process?&nbsp; As a "bonus," we will show that, at least for a binomial generating process, the answer is "Yes!"

Specifically, suppose that this probability $p$ register is used to count the number of "successes" resulting from a binomial experiment with underlying probability of each success equal to $P.~$ Again, it seems intuitive that using such a register shouldn't affect the binomial nature of the counting process, just the proportion of successes counted, i.e., the register's counting process is binomial with parameter $pP$; we will show that this is indeed the case.

Let $n$ be the number of successes registered, out of $N$ total trials, $j$ of which were actually successes.&nbsp; Then, necessarily, $n \le j \le N$, and, as with Problem 2.3, 

$P\{\text{successes registered} = n\} = P\{\text{successes registered} = n, \text{actual number of successes} = n\}$

$+~P\{\text{successes registered} = n, \text{actual number of successes} = n+1\}~+~... $

$+~P\{\text{successes registered} = n, \text{actual number of successes} = N\}$

$= \sum_{j=n}^N P\{\text{registered} = n \mid \text{actual successes} = j\} P\{\text{actual successes} = j\}$.  Now, setting $Q = 1-P$, the second factor is $\left(\!
  \begin{array}{c}
    N \\
    j
  \end{array}
  \!\right) P^jQ^{(N-j)}$, while the first factor is $\left(\!
  \begin{array}{c}
    j \\
    n
  \end{array}
  \!\right) p^nq^{(j-n)}$, with $q = 1-p$, as usual.&nbsp; Our sum is thus:

$\sum_{j=n}^N 
\left(\!
  \begin{array}{c}
    N \\
    j
  \end{array}
  \!\right) P^jQ^{(N-j)} 
\left(\!
  \begin{array}{c}
    j \\
    n
  \end{array}
  \!\right) p^nq^{(j-n)} = 
\sum_{j=n}^N \frac{N!}{j!(N-j)!} P^j Q^{(N-j)} \frac{j!}{n!(j-n)!} p^n q^{(j-n)}$

$= \sum_{j=n}^N \frac{N!}{(N-j)!(j-n)!n!} P^j Q^{(N-j)} p^n q^{(j-n)} = \sum_{j=0}^{N-n} \frac{N!}{(N-(j+n))! (j+n-n)! n!} P^{j+n} Q^{N-(j+n)} p^n q^{j+n-n}$

$= \sum_{j=0}^{M=N-n} \frac{(M+n)!}{(M-j)! j! n!} P^{j+n} Q^{M-j} p^n q^j = (pP)^n Q^M \sum_{j=0}^M \frac{M!(M+1)(M+2)...(M+n)}{j! (M-j)! n!} (Pq/Q)^j$

$= (pP/Q)^n Q^N 
\left(\!
  \begin{array}{c}
    N \\
    n
  \end{array}
  \!\right) 
\sum_{j=0}^{M} 
\left(\!
  \begin{array}{c}
    M \\
    j
  \end{array}
  \!\right) (Pq/Q)^j (1)^{M-j} = (pP/Q)^n Q^N 
\left(\!
  \begin{array}{c}
    N \\
    n
  \end{array}
  \!\right) 
(1 + Pq/Q)^M$ (by the binomial theorem)

$= (pP/Q)^n Q^N 
\left(\!
  \begin{array}{c}
    N \\
    n
  \end{array}
  \!\right) 
\left(\frac{Q + qP}{Q}\right)^M =
(pP/Q)^n Q^{N-(N-n)}
\left(\!
  \begin{array}{c}
    N \\
    n
  \end{array}
  \!\right) 
(Q+qP)^M =
\left(\!
  \begin{array}{c}
    N \\
    n
  \end{array}
  \!\right) 
(pP)^n (1-P + (1-p)P)^M$

$= \left(\!
  \begin{array}{c}
    N \\
    n
  \end{array}
  \!\right) 
(pP)^n (1 - pP)^{N-n}$. 

Thus, as claimed, a binomial experiment with parameter $P$ using an "imperfect success detector" that detects a success with probability $p$ has a binomial distribution with parameter $pP$.
