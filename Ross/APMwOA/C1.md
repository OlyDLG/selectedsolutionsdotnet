<style>
body {font-family: Palatino}
</style>
## <center> Selected Problem Solutions
<center>from</center>
### <center>Ross, S., 1992. <i>Applied Probability Models with Optimization Applications.</i>  Dover, New York
### <center>Chapter 1: Introduction to Stochastic Processes
### <center>&copy; 2016, 2025 by 
### <center>David Lawrence Goldsmith
<center>for</center>
## <center>[SelectedSolutionsDotNet](https://olydlg.github.io/selectedsolutionsdotnet/)

<i>Note:  These solutions are provided "as-is," for informational purposes only, with no warranty of any kind, expressed or implied, including that of correctness, adequacy, and/or suitability for any purpose, whatsoever.</i> Corrections are welcome and should be emailed to selectedsolutionsdotnet@gmail.com.

3) Given $X, Y$ independent Poisson RV's, find the distribution of $X + Y$.

Sln: We use the fact (pg. 3) that the characteristic function of a sum of independent RV's is the product of the characteristic functions of the summands. From the table on page 4, we have that $X \sim \text{POIS} (\lambda)$ has characteristic function $\phi_X(u) = \exp[\lambda(e^{iu}-1)]$, so for the sum of two independent Poisson RV's, we have: $$\phi_{X+Y}(u)=\phi_X(u)\phi_Y(u) = \exp[\lambda_X(e^{iu}-1)]\exp[\lambda_Y(e^{iu}-1)] = \exp[(\lambda_X + \lambda_Y)(e^{iu}-1)]$$ which is recognized to be the characteristic function of a Poisson RV with parameter $\lambda = \lambda_X + \lambda_Y$. In other words, the required distribution is $\boxed{\text{POIS}(\lambda_X + \lambda_Y)}$.

<br>
4) Given $X \sim \text{POIS}(\lambda_1), Y \sim \text{POIS}(\lambda_2)$, show that $(X \mid X + Y) \sim \text{BIN}(n,p)$ for some whole number $n$, and some real number $p : 0 \lt p \lt 1$. 

Pf: $P(X \mid X + Y) = P(X=x \mid X+Y=n\in \mathbb{W}) = P(X=x, X+Y=n)~/~P(X+Y=n)$ 
$= P(X=x, Y=n-x)~/~P(X+Y=n)$. Now, by 3), $P(X+Y = n)= e^{-(\lambda_1 + \lambda_2)}(\lambda_1 + \lambda_2)^n/n!$, so  $P(X=x, Y=n-x)~/~P(X+Y=n) =$ $$\frac{n!e^{(\lambda_1 + \lambda_2)}}{(\lambda_1 + \lambda_2)^n}\frac{e^{-\lambda_1}\lambda_1^x}{x!}\frac{e^{-\lambda_2}\lambda_2^{n-x}}{(n-x)!}=\left(\!
  \begin{array}{c}
    n \\
    x
  \end{array}
  \!\right)
  \left(\frac{\lambda_1}{\lambda_1 + \lambda_2}\right)^x
  \left(\frac{\lambda_2}{\lambda_1 + \lambda_2}\right)^{(n-x)}$$
$=~_nC_xp^x(1-p)^{(n-x)}$, with $p=\lambda_1 / (\lambda_1 + \lambda_2)$, and, given that $\lambda_1$ and $\lambda_2$ are both $\gt 0, 0 \lt p \lt 1.~~~\blacksquare$

<br>
5) Find the distribution of $X$, given that $X \sim \text{POIS}$ with $EX=\lambda \sim \text{EXP}$ with $E\lambda = 1/\mu$.

 Sln: The "trick" here is to recognize that the "distribution of $X$" being sought is the <i>marginal</i> distribution: $P_X(x) = \lim_{\lambda \rightarrow \infty} P\{X=x,\Lambda \le \lambda\}\text{ (Expression 1)}= \lim_{\lambda \rightarrow \infty}\int_{-\infty}^{\lambda}P\{X = x \mid \Lambda \le \lambda\}dP\{\Lambda \le \lambda\}$ (Expression 2). (If you're wondering how I got from Expression 1 (1) to the integrand in Expression 2 (2) using only the material in this Chapter, I'm sympathetic: the closest I can point to is the relationship, given on page 7, between a joint probability density, a corresponding conditional density, and the marginal density of the "conditioned-on" RV, $f_{Y \mid X}(y \mid x) = f_{X,Y}(x,y)/f_X(x)$, and appeal to the pre-requisite level stated in the book's Preface, but I agree, (1) to (2) is a bit of a stretch, given only the material Ross has given us in Chapter 1.) Now, $\lambda \gt 0$ implies that the bottom integration limit, $-\infty$, can be replaced with $0$, and $\lambda \sim \text{EXP}$ with $E\lambda = 1/\mu \Rightarrow dP\{\Lambda \le \lambda \} = \mu e^{-\mu\lambda}$ so (2) becomes:
 $$P_X (x) = \int_0^{\infty}\frac{\lambda^x e^{-\lambda} }{x!}\mu e^{-\mu\lambda} d\lambda =\frac\mu{x!}\int_0^{\infty}\lambda^x e^{-\lambda(1+\mu)}d\lambda = \frac\mu{x!}\frac{x!}{(1+\mu)^{x+1}}$$ (where, via Google Search, we've used Kirk A. Peterson's [online table of integrals, accessed 2015-11-14](http://tyr0.chem.wsu.edu/~kipeters/Chem332/resources/TableofUsefulIntegrals.pdf) for the value of the "improper" integral) $$= \frac\mu{1+\mu}\left(\frac1{1+\mu}\right)^x = p(1-p)^x \text{, where } p = \frac\mu{1+\mu} \in (0,1)~~(\text{since }\mu \gt 0)$$ $\\ \Rightarrow (1-p) \in (0,1) \Rightarrow$ $$\boxed{X \sim \text{GEOM}\left[\frac{\mu}{1+\mu}\right]}.$$  
 
<br>
6) An urn has $n$ chips [all distinguishable from one another...is what I'm going to assume; for an additional exercise, assume $n$ can be divided up, not necessarily equally, into $m \lt n$ equivalence classes of indistinguishable chips, with every chip in precisely one class]. Chips are drawn one at a time, [recorded], and then put back in the urn. Let $N$ denote the number of draws required UNTIL [em. added] some chip is drawn a second time. Find $P(N)$. 

Sln: $P_n(N=j,~j=1,…, n) \equiv P_n(N =~$number of draws before duplicate), so first we need to prove that $\{1,…, n\}$ is indeed the sample space, $S$. First, I submit as obvious that the sample space is discrete: one can only have an integer number of "draws" (including allowing for the possibility of "deposits"). Now, on the lower end, one must have at least 1 draw before a duplicate—there's no way to get a duplicate on one's 1^st draw, so the lower bound for $N$ is 1. On the top end, it is possible that one draws all $n$ distinct chips before one's first duplicate, but if that happens, the $n + $1^st draw is guaranteed to be a duplicate, so $n$ is the maximum number of draws possible (before a dupe). We assume—unless the derivation of $P_n$ indicates otherwise—that $j =$ number of draws, can equal any integer value between its just-shown minimum of 1 and maximum of $n$. Thus the enumeration of $S$ as {$1,…, n$} has been established. (Having established the primary importance of $n$ as defining the maximum value in $S$, we will now drop its subscripting of $P$ and define $P\{N=j\} =$ our prior notion of $P_n\{N=j\}$, i.e., the dependence  on some positive integer $n \ge 1$ is henceforth understood.)

Now, for $n$ distinguishable chips, the "event" $N=j$ occurs as follows: 
the first $j$ draws are all different from one another, the $j+1$^st draw repeats one of the previous $j$ results, and the rest of the draws—up to and including the $n$^th —can be any of the $n$ chips (all such sequences must be counted to get the total number of different ways $N=j$).  Likewise, to get said total, we need to count the different possibilities for the first $j$ draws as if order matters (drawing a blue chip before a red chip, say, has to be counted separately from drawing a red chip before a blue).  Thus the number of different ways for the first $j$ draws is $_nP_j$, the number of permutations of $n$ things taken $j$ at a time (when order matters) $\displaystyle= \frac{n!}{(n-j)!}$; the number of different, "satisfying" choices for the $j+1$^st draw is $j$ (i.e., one of the prior $j$ choices); and the number of "satisfying" choices for the last $n-(j+1)$ draws is $n^{n-(j+1)}$; putting it all together, we get that the total number of different ways of satisfying $N=j$ is $_nP_j j n^{n-(j+1)}$. On the other hand, the total number of distinguishable sequences of $n$ draws of $n$ objects, with replacement and order mattering, is simply $n^n$. Thus we finally arrive at: $$P\{N=j\} = \frac{n! j n^{n-(j+1)}}{(n-j)!n^n} = \boxed{\frac{n!}{(n-j)!}jn^{-(j+1)}}.$$

As a check, we confirm that our result is at least a valid probability density by evaluating $\sum_{j=1}^{n}P\{N=j\}  = $ 
$$\sum_{j=1}^{n} \frac{n!}{(n-j)!}jn^{-(j+1)} = 
\sum_{j=1}^{n} \frac{n!}{(n-j)!}(n-(n-j))n^{-(j+1)}~^{^{(*)}}$$ $$=\sum_{j=1}^{n-1}\left[ \frac{n!}{(n-j)!} n^{-j} - \frac{n!}{(n-j-1)!}n^{-(j+1)}\right] + \frac{n!}{0!}n^{-n} = \sum_{j=1}^{n-1}\left(A_j - B_j\right) + \frac{n!}{n^n},$$
where $A_j = \displaystyle\frac{n!}{(n-j)!}n^{-j},~B_j = \frac{n!}{(n-j-1)!}n^{-(j+1)}$. We now observe that $B_{j-1} = n!/(n-(j-1)-1)!n^{-(j-1+1)} = n!/(n-j)!n^{-j} = A_j$ and thus rewrite our last result as 
$$\sum_{j=1}^{n-1}\left(A_j - B_j\right) + \frac{n!}{n^n} = \sum_{j=1}^{n-1}\left(B_{j-1} - B_j\right) + \frac{n!}{n^n} = B_0 - B_1 + B_1 - B_2 + … + B_{n-2} - B_{n-1} + \frac{n!}{n^n}$$
$$= B_0 - B_{n-1} + \frac{n!}{n^n} = \frac{n!}{(n-1)!}n^{-1} - \frac{n!}{(n-(n-1)-1)!}n^{-(n-1+1)} + \frac{n!}{n^n} = \frac{n!}{n!} - \frac{n!}{0!}n^{-n} + \frac{n!}{n^n} = 1~~\checkmark$$ 

(* Thanks to [Ankush Jain, via Quora](http://qr.ae/Rg2dmZ), for pointing out that the $j = n-(n-j)$ "trick" would work to complete this derivation.)

<br>
9) Let $N$ denote the number of customers arriving at a store in a given day. Suppose that the amounts spent by the customers are independent and have a [common] distribution $F$. Find the mean and variance of the total amount of money spent in the store [per day].

Sln: We assume that $N$ is distributed Poisson with parameter $\lambda = EN$, and that $EF = \mu$ is the expected expenditure per customer.  Then the expected income from $n$ customers is $n\mu$ and the expected daily income = $\sum_{n=0}^{\infty}n\mu P(N=n) = \mu\sum_{n=0}^{\infty}ne^{-\lambda}\lambda^n/n! = \mu\sum_{n=1}^{\infty} e^{-\lambda}\lambda^n/(n-1)! = $<br>$\mu\sum_{n=0}^{\infty}e^{-\lambda}\lambda^{n+1}/n! = \mu\lambda\sum_{n=0}^{\infty}e^{-\lambda}\lambda^n/n! = $ $$\boxed{\mu\lambda},$$ i.e., the expected number of customers per day times the expected expenditure per customer, just as intuition would suggest.

To compute the variance, we need to evaluate $E(X^2) - (EX)^2$, where $X$ represents the (random) income per day. We already know $(EX)^2 = (\mu\lambda)^2$, so we are left to evaluate $E(X^2) = $<br><br>$\sum_{n=0}^{\infty}(n\mu)^2P(N=n) = \mu^2\sum_{n=0}^{\infty}n^2e^{-\lambda}\lambda^n/n! = \mu^2\left[\lambda e^{-\lambda} + \sum_{n=2}^{\infty}ne^{-\lambda}\lambda^n/(n-1)!\right] = $<br><br>$\mu^2\left[\lambda e^{-\lambda} + \sum_{n=0}^{\infty}(n+2)e^{-\lambda}\lambda^{n+2}/(n+1)!\right] = \mu^2\lambda\left[e^{-\lambda} + \sum_{n=0}^{\infty}((n+1)+1)e^{-\lambda}\lambda^{n+1}/(n+1)!\right] =$<br><br>$\mu^2\lambda\left[e^{-\lambda} + \sum_{n=0}^{\infty}(n+1)e^{-\lambda}\lambda^{n+1}/(n+1)! + \sum_{n=0}^{\infty}e^{-\lambda}\lambda^{n+1}/(n+1)!\right] = $<br><br>$\mu^2\lambda\left[e^{-\lambda} + \lambda\sum_{n=0}^{\infty}e^{-\lambda}\lambda^n/n! + \sum_{n=1}^{\infty}e^{-\lambda}\lambda^n/n!\right] = \mu^2\lambda\left[e^{-\lambda} + \lambda(1) + (1-e^{-\lambda})\right] = \mu^2\lambda(\lambda + 1) \therefore$ 
$$\text{Var}(X) = \mu^2\lambda^2 + \mu^2\lambda - (\mu\lambda)^2 = \boxed{\lambda\mu^2}.$$

<br>
10) Show that any Process with independent increments is Markov.

Sln: We need to show that assuming independent increments is sufficient to ensure that $P\{X \le x \mid X_1 \le x_1,  X_2 \le x_2, …, X_n \le x_n\} = P\{X \le x \mid X_n \le x_n\}~\forall t > t_n >t_{n-1}>…>t_2>t_1$. By induction on $P\{X \mid Y\} = P\{X, Y\}/P\{Y\}$ it can be established that $P\{X \mid X_1, X_2, …, X_n\} = P\{X, X_1, X_2,…, X_n\}/\prod_{i=1}^{n}P\{X_i\}$, thus $P\{X \le x \mid X_1 \le x_1,  X_2 \le x_2, …, X_n \le x_n\} = P\{X \le x, X_1 \le x_1,  X_2 \le x_2, …, X_n \le x_n\}/\Pi,~\text{where}~\Pi = \prod_{i=1}^{n}P\{X_i \le x_i\}$. Now the condition of independent increments is stated for us as: $\forall~t_0 < t_1 <…< t_n,$ the $n$ random variables $X_1 - X_0, X_2 - X_1, …, X_n - X_{n-1}$ are all mutually independent; since $X \le x, Y \le y \Rightarrow X - Y \le x - y$, we have that $P\{X \le x, X_1 \le x_1,  X_2 \le x_2, …, X_n \le x_n\}/\Pi =$ $P\{X - X_n \le x - x_n, X_n - X_{n-1} \le x_n - x_{n-1}, …, X_2 - X_1 \le x_2 - x_1\} / \Pi = $ (because the assumption of independent increments $\Rightarrow$ the probability of all those inequalities being true simultaneously is the product of the probabilities of each of them being true separately) $P\{X-X_n \le x - x_n\} \cdot \prod_{i=1}^{n-1} P\{X_{i+1} - X_i \le x_{i+1} - x_i\} / \Pi =$
<br>
$P\{X-X_n \le x - x_n\} \cdot \prod_{i=1}^{n-1} P\{X_i \le x_i\} / \Pi =$ (because $\Pi = P\{X_n \le x_n\} \cdot \prod_{i=1}^{n-1} P\{X_i \le x_i\}$)
<br>
$P\{X-X_n \le x - x_n\} / P\{X_n \le x_n\}$ $=$
<br>
$P\{X \le x, X_n \le x_n\} / P\{X_n \le x_n\} = $
<br>
$P\{X \le x \mid X_n \le x_n\}.~~~\blacksquare$

<br>
11) Let $Y_{1,n}, Y_{2,n},…, Y_{n,n}$ be $n$ independent random variables with identical [continuous] uniform distibution on $(0,t)$ [i.e., relative to the notation used in the text's Table 2, $a = 0$ and $b = t$]. Let $Z_n = \min(Y_{1,n},…, Y_{n,n})$.

a) Find $P\{Z_n > x\}$.

Sln: $P\{\min (Y_{1,n},…, Y_{n,n}) > x\} = P\{Y_{1,n} > x, Y_{2,n} > x,…, Y_{n,n} > x\}$ and since the $Y_{i,n}$ are all independent of one another and identically distributed, this last equals $(P\{Y_{,n} > x\})^n = (1 - P\{Y_{,n} \le x\})^n =  (1 - \int_0^x\frac1t d\xi)^n = $ $$\boxed{\left(1 - \frac xt \right)^n}.$$

b) Let $t$ be a function of $n$ such that $\lim_{n\rightarrow \infty}n/t = \lambda$. Show that $\lim_{n \rightarrow \infty} P\{Z_n > x\} = e^{-\lambda x}.$ 

Sln: $\lim_{n \rightarrow \infty} n/t(n) = \lambda \Rightarrow \lim_{n \rightarrow \infty} t(n) \sim n/\lambda$.  Substituting this in for $t$ in the result for part a) and taking the limit as $n \rightarrow \infty$ yields $\lim_{n \rightarrow \infty} (1 - \lambda x / n)^n$, which, as a standard result from Calculus (or evaluate the limit of the logarithm of the expression using L'Hopitals' Rule to find that it equals $-\lambda x$), equals $e^{-\lambda x}$, as claimed.

<br>
13) Show for the Wiener process that Var$~X(t) = \sigma^2 t$ for some $\sigma ^2 > 0.$

$\text{Var}~X(t) = E(X^2(t)) - [E(X(t))]^2$, and since $X(t)$ is a Wiener process, $E(X(t)) = 0~\forall t>0$ so $\text{Var}~X(t) = E(X^2(t)) = \int_{0}^{t} dt \int_{-\infty}^{\infty}x^2 \exp(-x^2/(2\sigma ^2))/\sqrt{2\pi \sigma^2} dx = t(\text{Var}(\text{Normal}(0, \sigma^2))) = \sigma^2 t.~~~\blacksquare$

### Please Donate:
<table>
  <tr style="border: none; background: transparent;">
    <td style="border: none;">
      <b>Venmo: @David-Goldsmith-13</b>
    </td>
    <td style="border: none;">
      <form action="https://www.paypal.com/cgi-bin/webscr"
            method="post"><input name="cmd"
            value="_xclick" type="hidden"> <input name="business"
            value="dgoldsmith_89@alumni.brown.edu" type="hidden"> <input
            name="item_name" value="SelectedSolutions Donation"
            type="hidden"> <input name="cn" value="Special Instructions
            (optional" type="hidden"> <input
            src="https://www.paypal.com/images/x-click-but04.gif"
            name="submit" alt="Make payments with PayPal - it's fast,
            free and secure!" align="middle" border="0" type="image"></form>
    </td>
  </tr>
</table>
