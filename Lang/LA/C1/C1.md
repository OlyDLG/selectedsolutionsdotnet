<style>
body {font-family: Palatino}
</style>

## <center>Selected Exercise Solutions
<center>from</center>
### <center>Lang, S., 2004, <i>Linear Algebra, Third Edition, Corrected Printing</i>, Springer, New York.
### <center>Chapter 1: Vector Spaces
### <center>&copy; 2019, 2025 by
### <center>David Lawrence Goldsmith
<center>for</center>
## <center>[SelectedSolutionsDotNet](https://olydlg.github.io/selectedsolutionsdotnet/)

<i>Note 1:  These solutions are provided "as-is," for informational purposes only, with no warranty of any kind, expressed or implied, including that of correctness, adequacy, and/or suitability for any purpose, whatsoever.</i> Corrections are welcome and should be emailed to selectedsolutionsdotnet@gmail.com.

<i>Note 2: Though in other of our works we distinguish vectors with boldface font, in the present work, because of their frequency of occurrence, we herein adopt Prof. Lang’s convention&mdash;in this text&mdash;of using un-bolded, "late in the alphabet" letters (e.g., u, v, w, x, y, z, and Greek equivalents) to denote vectors. Indeed, we will follow Prof. Lang’s notational conventions as closely as possible, including the use of his abbreviations for key properties, e.g., "SP#" for "scalar product (property) number," "VS#" for "vector space (property) number," etc.; much more frequently than Prof. Lang, however, we will use standard symbols for words (e.g., "$\because$" for</i>  since/because, "$\therefore$" <i>for</i> therefore/thus/as a consequence, "$\forall$" <i>for</i> for all/each, "$\exists$" <i>for</i> there exists/is, <i>etc.) Other less-standard abbreviations we shall use include</i> "Pf" <i>for</i> "proof," "PoF" <i>for</i> "property of fields," "$\overset{\rightharpoonup}{\subset}$" <i>for</i> "is a (vector) subspace of,"<i> etc.</i>

<i>Note 3: For a typical post-Calculus student, it is not uncommon that the more theoretical (as opposed to a more calculations-and-applications-focused) treatment of Linear Algebra, such as Prof. Lang gives us in this text, is a student’s first substantial encounter with being asked to create detailed, rigorous proofs as assigned exercises. (Such a student might have been previously expected to follow such proofs, e.g., given in lecture or in a text, and then, perhaps, "regurgitate" them on an exam, but, in my experience at least, Linear Algebra is a common first place where one is expected to create them, "from scratch," on one’s own.) Therefore, it should surprise no one that students, even considerably "bright" ones, frequently find this new requirement to be significantly more difficult than any other they’ve previously had in studying mathematics. This is the primary motivation for presenting proofs of these elementary theorems, so detailed (e.g., explicitly including justification for each step) that at times it may seem pedantic to a more advanced reader.</i>

<br>
### Section 1: Definitions
<b>0)</b> (From the "innards" of the text): Let $v \in V, V$ a vector space; show that $(-1)v = -v$ (i.e., the underlying field’s additive inverse of $1$, multiplied by any vector in $V$, yields that vector’s additive inverse in $V$).

<b>Pf</b>: (Note: here and subsequently, the notation $X\underset{A}{=}Y$ means $A$ is the reason $X = Y$, and, similarly, $P \underset{A}{\implies} Q$ means $A$ is the reason $P$ implies $Q$.)

$O \underset{\text{Pf in text}}{=} 0v \underset{\text{PoF}}{=} (1+(-1))v \underset{\mathbf{VS6~\&~8}}{=} v + (-1)v$; now add $-v$ to both sides to get $-v + O = -v + v + (-1)v$; by <b>VS2</b> the left-hand-side (LHS) equals $-v$ and by <b>VS1</b> the RHS equals $O + (-1)v \underset{\mathbf{VS2}}{=} (-1)v.~~~\blacksquare$
<br><br>

<b>1)</b> Given $c \in K, K$ the field underlying a vector space $V$; show that $cO_{_V} = O_{_V}$.

<b>Pf</b>: Let $v \in V\text{; by } \mathbf{VS3},~\exists (-v) \in V : -v + v = O_{_V}$, (":"  means "such that") $\therefore cO_{_V} \underset{\text{substitution}}{=} c(-v + v) \underset{\mathbf{VS5}}{=} c(-v) + cv \underset{\text{Exercise 0}}{=} c(-1(v)) + cv \underset{\mathbf{VS7}~\text{& PoF}}{=} (-c)v +cv \underset{\mathbf{VS6}}{=} (-c + c)v \underset{\text{PoF}}{=} 0v \underset{\text{Pf in text}}{=} O_{_V}.~~~\blacksquare$
<br><br>

<b>2)</b> Given $c \in K, c \ne 0, v \in V_{_K}$; show that $(cv = O) \implies v = O$.

<b>Pf</b>: Since $c \ne 0, \displaystyle \frac1c  \underset{\text{PoF}}{\in} K \therefore O \underset{\text{Exercise 1}}{=} \frac1c O \underset{\text{by assumption}}{=} \frac1c(cv) \underset{\mathbf{VS7}}{=} \left(\frac{c}c\right)v \underset{\text{PoF}}{=} (1)v \underset{\mathbf{VS8}}{=} v.~~~\blacksquare$
<br><br>

<b>4)</b> Given $v, w \in V$; show that $(v+w = O) \implies w=-v$, i.e., the additive inverse of $v$ is unique.

<b>Pf</b>: $v + w \underset{\text{by assumption}}{=} O \underset{\mathbf{VS3}}{=} v + (-v)$; now add $-v$ to both sides: $\left[-v + (v + w) = -v + (v + (-v))\right]  \underset{\mathbf{VS1}}{\implies} \left[(-v + v) + w = (-v + v) + (-v)\right] \underset{\mathbf{VS3~\&~4}}{\implies} \left[O + w = O + (-v)\right] \underset{\mathbf{VS2}}{\implies} w = -v.~~~\blacksquare$
<br><br>

<b>7)</b> Let $A_{i=1,2,..,r} \in \mathbb{R}^n, W = \{B \in \mathbb{R}^n : B \cdot A_i = 0~\forall i\in \{1,2,...,r\} \}$; show that $W\overset{\rightharpoonup}{\subset} \mathbb{R}^n$.

<b>Pf</b>: Unlike proving that a set is a vector space, which requires establishing ten properties ("closure" of vector addition and multiplication by a scalar, as well as VS 1-8), in order to prove that a subset, $W$, of a vector space, $V_{_K}$, is a subspace (i.e., is itself a vector space), all one need do is establish three properties: i) $v, w \in W \implies v + w \in W$; ii) $v \in W, c \in K \implies cv \in W$; and iii) $O_{_V} \in W$. In general, how does one do these things? Well, how do we know if something is an element of $W$ in the first place? Generally speaking, we are given some "rule" which allows us to say definitively whether or not an object is in the set&mdash;this is the tacit part of all of the above: we need to establish that the object following the implication, e.g., $v +w, cv, O_{_V}$, satisfies whatever rule(s) have been specified to decide whether the object is in the set.

In this case, the rules are that: a) the object is in $\mathbb{R}^n$; and b) the dot product of the object with <i>each</i> of the $A_i$ is zero. For each of i), ii), and iii), a) follows from $W \subset \mathbb{R}^n \because \mathbb{R}^n$ is a vector space. Thus this all boils down to showing that if $v,w$ are orthogonal to each of the $A_i$, then $v+w$ and $cv$ are also (as well as that $O_{_V} \cdot A_i = 0~\forall i$, which is trivially so). For $v+w$ and each of the $A_i$, we have $(v+w)\cdot A_i \underset{\mathbf{SP2}}{=} v \cdot A_i + w \cdot A_i \underset{v,w \in W}{=} 0 + 0 = 0 \therefore v+w \in W$. Similarly, $cv \cdot A_i \underset{\mathbf{SP3}}{=} c(v \cdot A_i) \underset{v\in W}{=} c(0) = 0 \therefore cv \in W.~~~\blacksquare$
<br><br>

<b>9b)</b> Show that the subset, $X$, of $\mathbb{R}^3$ given by $X = \{(x,y,z): x = y, 2y = z\}$ is a subspace thereof.

<b>Pf</b>: $O_{\mathbb{R}^3} = (0,0,0) \in X \because 0 = 0$ and $2(0) = 0$; $(x,y,z) \in X$ and $c \in \mathbb{R}\implies c(x,y,z) \in X \because x=y \implies cx = cy$ and $2y = z \implies 2cy = cz$; and $(x,y,z), (\xi, \psi, \zeta) \in X \implies (x+\xi, y+\psi, z+\zeta) \in X \because x=y, \xi=\psi \implies x + \xi = y + \psi$ and $2y = z, 2\psi = \zeta \implies 2(y+\psi) = z+\zeta.~~~\blacksquare$ 
<br><br>

<b>10)</b> Given $U, W \overset{\rightharpoonup}{\subset} V$ (over a field $K$), show that $U \cap W$ and $U + W$ are also.

<b>Pf</b>: We will prove these separately; first, $U \cap W$: $\because U, W \overset{\rightharpoonup}{\subset} V, O_V \in U, W \therefore O_V \in U \cap W$; next, suppose $u, w \in U \cap W, c \in K$; the former implies both $u, w \in U, W$ and $\because U, W$ are vector spaces, $u+w$ and $cu \in U$ and $\in W, \therefore u+w$ and $cu \in U \cap W$.

For $U+W$, first note that, $\because U, W \overset{\rightharpoonup}{\subset} V, O_U + O_W = O_V + O_V = O_V \therefore O_V \in U+W$. Next, suppose $\upsilon, \omega \in U+W$; this $\implies \exists u_1, u_2 \in U, w_1, w_2 \in W : \upsilon = u_1 + w_1, \omega = u_2 + w_2$; now consider $\upsilon + \omega = u_1 + w_1 + u_2 + w_2 = (u_1 + u_2) + (w_1 + w_2); \because U, W$ are vector spaces, $u_1 + u_2 \in U, w_1 + w_2 \in W, \therefore (\upsilon + \omega) \in U+W$; finally, $\upsilon \in U+W \implies \exists u \in U, w \in W : \upsilon = u+w \therefore c\upsilon~(c \in K) = cu + cw$ and again, since $U, W$ are vector spaces, $cu \in U, cw \in W \therefore c\upsilon \in U+W.~~~\blacksquare$
<br><br>

<b>11)</b> Given $K$ a subfield of field $L$, show that $L$ is a vector space over $K$ (implying, e.g., that $\mathbb{C}$ is a vector space over $\mathbb{R}$ and over $\mathbb{Q}$, and that $\mathbb{R}$ is a VS over $\mathbb{Q}$).

<b>Pf</b>: This is all-but-trivial: the only thing we need to "worry about" is to be clear regarding what is vector addition, $\overset{V}{+}$, and multiplication of a vector by a scalar, $k\overset{V}{\cdot}$ in $L$ *as a vector space*, and the reason we should be careful about this is because, in this situation, the required definitions are rather different than what we’re used to. To wit: we’re used to vectors consisting of $n$-tuples of scalars from the underlying field, and then defining vector addition by component-wise scalar (i.e., the field’s) addition, and similarly for multiplication of a vector by a scalar; but in the present situation, we don’t have *any* specified "construction" of the elements of the vector space, only the specification that, among the elements, the algebraic laws of being a field, apply. Thus, the "obvious" choice for vector addition, $\overset{V}{+}$, *is* the field addition, $+$, and the "obvious" choice for multiplication of a vector by a scalar, $k\overset{V}{\cdot}$, *is* the field multiplication, $k\cdot$&mdash;let’s see if this works.

We need to show:

A) $\lambda, \mu \in L \implies \lambda \overset{V}{+} \mu \in L$: by choosing the field $L$’s addition for the vector addition $\overset{V}{+}$, this follows immediately from $L$ being a field;

 B) $\lambda \in L, k \in K \implies k \overset{V}{\cdot}\lambda \in L$: this follows immediately from $L$ being a field by choosing $L$’s multiplication for $k\overset{V}{\cdot}$ (and the fact that $k \in K \implies k \in L \because K \subset L$);
 
VS1 through VS8 all follow similarly, i.e., the above choices for vector addition and multiplication of a vector by a scalar make VS1-8 follow immediately from $L$ being a field. $~~\blacksquare$ 
<br><br>

<b>14)</b> Given $c \in \mathbb{Q}, c \gt 0, \gamma \in \mathbb{R} : \gamma^2 = c$; show that $S = \{x: x= a + b\gamma, a,b \in \mathbb{Q} \}$ is a field.

<b>Pf</b>: We need to show: a) $x,y \in S \implies x+y \in S$ and $xy \in S$; b) $x \in S \implies -x \in S$ and $x^{-1} \in S$;  and c) $0,1 \in S$. The primary issue here is that of "closure": certainly the sum and product of any two real numbers, $x,y$, each of which can be written as $a+b\gamma$ for compliant $a,b,\gamma$ is again a real number; the question is, are $x+y$ and $xy$ also elements of $S$, i.e., can they also be written in the form $a+b\gamma$ for compliant $a, b$, and $\gamma$. Similarly, is it always true that $-x$ and $x^{-1}$ can be so written.

The easiest of the three things we need to show is c): $0 \in S \because 0 = 0+0\gamma$ for any compliant $\gamma$ (since it is true for any $\gamma$); likewise, $1 \in S \because 1 = 1+0\gamma$. Continuing in reverse order, i.e, with part b): $x \in S \implies \exists a,b \in \mathbb{Q}, \gamma \in \mathbb{R}: \gamma^2=c \in \mathbb{Q}^+ : x=a+b\gamma$. Consider $y=-a + (-b)\gamma; \because \mathbb{Q}$ is a field, $-a,-b \in \mathbb{Q}$ and thus $y \in S$; furthermore, $x+y = a+b\gamma + (-a) + (-b)\gamma = [a+(-a)] + [b+(-b)]\gamma$ ($\because \mathbb{R}$ is a field) $= 0 + 0\gamma = 0$ (ditto) $\therefore y = -x \in S$. Now assume $x = a+b\gamma \ne 0$; this implies that $\gamma \ne -a/b \implies \gamma^2 = c \ne a^2/b^2 \implies a^2-b^2c \ne 0$, and consider $z = \displaystyle \frac{a}{a^2-b^2c} + \frac{-b}{a^2-b^2c}\gamma; z \in S$ (why?) and $xz = (a+b\gamma)\displaystyle \left[\frac{a}{a^2-b^2c} + \frac{-b}{a^2-b^2c}\gamma\right] = \frac{(a+b\gamma)(a-b\gamma)}{a^2-b^2c} = \frac{a^2-b^2c}{a^2-b^2c} = 1 \therefore z = x^{-1} \in S$. Finally, $x,y \in S \implies \exists a,b,p,q \in \mathbb{Q}$ and compliant $\gamma : x=a+b\gamma, y=p+q\gamma$, and thus $x+y = (a+b\gamma) + (p+q\gamma) = (a+p) + (b+q)\gamma$ (why?) $\in S$ (ditto) and $xy = (a+b\gamma)(p+q\gamma) = (ap + bqc) + (aq+bp)\gamma \in S$ (again, why?)

$\blacksquare$
<br><br>

### Section 2: Bases
<!---
__4__) Let $U=(a,b), V=(c,d) \in \mathbb{R}^2$. Show: __A__) $ad-bc=0 \implies U,V$ are linearly dependent; and __B__) $ad-bc \ne 0 \implies U,V$ are linearly independent (i.e., $ad-bc \ne 0$ is both a necessary and a sufficient condition for $U,V$ to be linearly independent).
__PfA&B__: To Revise
<!---By definition, $U,V$ are linearly dependent if there exist real numbers $x,y$, ___not both zero___, such that $xU+yV = x(a,b) + y(c,d) = (0,0)$. We must consider several cases, corresponding to the various ways in which $ad-bc=0$ can hold.
Case $a=b=c=d=0$: in this case, any two non-zero real numbers will do, e.g., $x=y=1$;
Case precisely one of $a,b,c,d \ne 0$: suppose it’s $a \ne 0$ (treatment of the other sub-cases of it being one of the others is exactly analogous); then $x=0, y=1$ are two real numbers, not both zero, such that $x(a,b)+y(c,d) = (0,0)$;
Case precisely two of $a,b,c,d = 0$: note that this case excludes $a=d=0$ and $b=c=0$ (because if $a=d=0$, then $ad-bc=0 \implies b=0$ or $c=0$ and we’re not actually in this case, and similarly if $b=c=0$); thus this case consists of:
$~~~$Sub-case $a=b=0, c,d \ne 0$: in this sub-case, $x=1,y=0$ suffices;
$~~~$Sub-case $a=c=0, b,d \ne 0$: in this sub-case, choose $x=-d, y=b$; then $xU+yV = -d(0,b)+b(0,d) = (0+0,-bd+bd) = (0,0)$;
$~~~$Sub-case $c=d=0, a,b \ne 0$: in this sub-case, $x=0,y=1$ suffices;
$~~~$Sub-case $b=d=0, a,c \ne 0$: in this sub-case, choose $x=-c, y=a$; then $xU+yV = -c(a,0)+a(c,0) = (-ac+ac,0+0) = (0,0)$;
Case precisely one of $a,b,c,d = 0$: this case is impossible if $ad-bc=0$ (why?);
Case none of $a,b,c,d=0$: in this case $ad-bc=0$ implies, for example, $d=\frac{bc}a, b=\frac{ad}c$, and thus if we choose $x=c, y=-a$, then $xU+yV = c(a,\frac{ad}c)-a(c,\frac{bc}a) = (ac-ac,ad-bc) = (0,0).~~~\blacksquare \text{A}$
__PfB__: By definition, two vectors are linearly independent if they are not linearly dependent, therefore, by elementary formal logic $(ad-bc \ne 0 \implies U,V \text{ linearly independent }) \iff (U,V \text{ linearly dependent } \implies ad-bc=0),$ i.e., __B__) is just asking us to prove the converse of __A__). Now, $U=(a,b), V=(c,d) \in \mathbb{R}^2$ linearly dependent means there exist real numbers $x,y,$ not both zero, such that $x(a,b)+y(c,d)=(0,0)$.
Case precisely one of $x,y = 0$: without loss of generality (why?), we may assume $x=0, y \ne 0$; then $0(a,b)+y(c,d)=(0,0) \implies c=d=0 \implies ad-bc=0$;
Case neither $x,y = 0$: in this case, the hypothesis that $U,V$ linearly dependent implies that the two equations $$\begin{eqnarray}ax+cy&=&0~~~~~(1)\\bx+dy&=&0~~~~~(2)\end{eqnarray}$$ must be simultaneously true; if $a=0$ or $c=0$, Eq. 1 implies that the other of these two must also equal zero $\implies ad-bc=0$, and similarly for $b=0$ or $d=0$; consequently, let as assume that $a \ne 0$ and $b \ne 0$ and multiply Eq. 1 by $-b$ and Eq. 2 by $a$, yielding:
$$\begin{eqnarray}-abx-bcy&=&0\\abx+ady&=&0\end{eqnarray}$$ adding these eliminates $x$, giving $(ad-bc)y=0$; since this case has assumed $y\ne 0$, we conclude that $ad-bc=0.$<br>
$\blacksquare$
<br--->

<br>
__9__) Given: $A_1,...,A_r \in \mathbb{R}^n : \forall i,j \in \{1,...,r\}, A_i \ne O, A_i \cdot A_{j\ne i} = 0$. Show: the $A_i$ are linearly independent.

__Pf__: We must show that $a_1A_1~+..a_iA_i..+~a_rA_r = O \implies a_i = 0~\forall i \in \{1,...,r\}$. Since $O \cdot A_j =0$, $0 = (a_1A_1~+..a_iA_i..+~a_rA_r) \cdot A_j = a_1A_1 \cdot A_j~+ ..a_iA_i \cdot A_j.. +~a_rA_r \cdot A_j = a_jA_j \cdot A_j$, since $A_i \cdot A_j = 0$ except for $i=j$. Since $A_j \ne O~\forall j \in \{1,...,r\}, A_j \cdot A_j \ne 0$, therefore $a_jA_j \cdot A_j = 0 \implies a_j = 0$. This is true for all $j \in \{1,...,r\}.~~~\blacksquare$
<br><br> 

__10__) Given linearly dependent $v \ne O, w \in V$ a vector space (over a field $K$), show that $\exists a (\in K) : w=av$.

__Pf__: If $w=O$ (which is trivially linearly dependent on all elements of $V$; why?) then $a=0$ suffices, so assume that $w \ne O$ (also). Now, $v,w$ linearly dependent implies $\exists p,q \in K$, not both zero, such that $pv+qw=O$; without loss of generality (why?), assume that $q \ne 0$; then we have: $O = pv+qw = q(\frac{p}q v+w)$ $\implies \frac{p}q v+w = O$ $\implies w = -\frac{p}q v$ and since $K$ is a field, $-\frac{p}q \in K.~~~\blacksquare$

<br>
### Section 3: Dimension of a Vector Space

No Exercises

<br>
### Section 4: Sums and Direct Sums

__2__) Given $V = \mathbb{K}^3$ for some field $\mathbb{K}, W$ the subspace generated by $(1,0,0), U$ the subspace generated by $(1,1,0)~\&~(0,1,1)$. Show that $V=W \oplus U$.

__Pf__: We can do this in either of two ways: 1) by appeal to the definition, i.e., "$V = W \oplus U$ if $\forall v \in V~\exists$ unique $w \in W$ and $u \in U$ such that $v=u+w$"; or 2) using Theorem 4.1, i.e., by showing that $V=U+W$ and that $U \cap W = \{O\}$; here we shall opt for strategy one, and save option two for the next exercise.

We must show that given an element $(v_1,v_2,v_3)$ of $V$, there exists a unique element $\omega = w(1,0,0)$ of $W$ and a unique element $\upsilon = u_1(1,1,0)+u_2(0,1,1)$ of $U :$ $(v_1,v_2,v_3) = \omega + \upsilon = w(1,0,0) + u_1(1,1,0)+u_2(0,1,1)$. This gives the simultaneous linear system: 
$$\begin{eqnarray}
w&+&u_1&   &      &=&v_1&~~(1) \\
   &   &u_1&+&u_2&=&v_2&~~(2)\\
   &   &      &   &u_2&=&v_3&~~(3)
\end{eqnarray}$$

Eq. 3 immediately gives $u_2 = v_3$; substituting this into Eq. 2 gives $u_1 = v_2 - v_3$, and substituting this into Eq. 1 yields $w = v_1-v_2+v_3$. Since $v_1,v_2,v_3$ are elements of a field, these $w, u_1, u_2$ are well- and uniquely defined.$~~~\blacksquare$
<br><br>

__3__) Given $A=(a_1,a_2), B=(b_1,b_2) \in \mathbb{R}^2 \ne O : \nexists c \in \mathbb{R} : cA=B$; show that $A,B$ form a basis for $\mathbb{R}^2$ and that $\mathbb{R}^2 = A’ \oplus B’$, where $A’$ (resp. $B’$) is the sub-space generated by $A$ (resp. $B$).

__Pf__: __Basis__: To show that $A,B$ constitute a basis for $\mathbb{R}^2$, we must show that they are linearly independent, and that they generate $\mathbb{R}^2$.

$~~~$__Linear Independence__: By Section 2, Exercise 10 (see above), if $A,B$ were linearly dependent, there would exist a constant $c$ such that $cA=B$, contradicting our hypothesis, therefore $A,B$ must be linearly independent.

$~~~$__Generation__: We need to show that $\forall v=(x,y) \in \mathbb{R}^2, \exists p,q \in \mathbb{R} : v = pA+qB$, i.e., there exists a solution of the simultaneous linear system: $$\begin{eqnarray}pa_1+qb_1&=&x \\ pa_2+qb_2&=&y\end{eqnarray}$$

Multiplying the first equation through by $-a_2$, the second equation through by $a_1$ and adding the two yields: $(a_1b_2-a_2b_1)q=a_1y-a_2x$ and similarly we obtain $(a_1b_2-a_2b_1)p=b_2x-b_1y$. Now, by Exercise 4 of Section 2 (see above), $(a_1,a_2), (b_1,b_2)$ linearly independent implies $a_1b_2-a_2b_1 \ne 0$ therefore we may divide to obtain $p=(b_2x-b_1y)/(a_1b_2-a_2b_1), q=(a_1y-a_2x)/(a_1b_2-a_2b_1).$

__Direct Sum__: $A,B$ a basis of $\mathbb{R}^2 \implies A’+B’=\mathbb{R}^2$ (why, exactly?) so by Theorem 4.1, we just need to show that $A’ \cap B’ = \{O\}$. Suppose not, i.e., suppose $\exists x \in \mathbb{R}^2 : \exists a,b \in \mathbb{R} : x=aA=bB$; then $aA-bB = O$, but by the linear independence of $A,B, a=b=0$, i.e., $x=O.$<br>$\blacksquare$
<br>

### Please Donate:
<form action="https://www.paypal.com/cgi-bin/webscr"
          method="post"><input name="cmd"
            value="_xclick" type="hidden"> <input name="business"
            value="dgoldsmith_89@alumni.brown.edu" type="hidden"> <input
            name="item_name" value="SelectedSolutions Donation"
            type="hidden"> <input name="cn" value="Special Instructions
            (optional" type="hidden"> <input
            src="https://www.paypal.com/images/x-click-but04.gif"
            name="submit" alt="Make payments with PayPal - it's fast,
            free and secure!" align="middle" border="0" type="image"></form>



